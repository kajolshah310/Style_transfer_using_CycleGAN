# Style_transfer_using_CycleGAN - Computer Vision
Multi-collection Style Transfer Using CycleGAN

Style transfer is the process of redrawing or generating a new image that combines the content or scene from one image and the artistic style from a different image or a collection of images. Initially, the problem first stemmed from wanting to make an input image look as if it was drawn with the same style as in a painting from a famous artist. Manually transferring the image to a different style by interpreting another artist’s style/technique is something that would take a professional artist considerable time and effort, if they are even able to achieve adequate results.
To automate the process, AI and computer vision techniques have been used to obtain promising results. As there are many practical applications for style transfer, such as quickly creating animations from real life scenes, there has been a lot of research and development into creatingmore accurate models that produce clear images and are pleasing to the eye. Unfortunately, many implementations of GANs that are able to obtain collective style transfer are only capable of generating images of one style. Therefore, multiple networks and models must be trained in order to achieve multiple models.

GANs utilize a deep learning approach in which two neural networks compete with each other.The first neural network is called a generative network. The generative network, after being trained, will be able to generate an image with the content from an input image and the style learned from a collection of artwork during training. The discriminator is the second neural network in the framework. The generative and discriminative networks are constantly in an
optimized two player game during training, where the discriminative network is responsible for determining/classifying if the input is painted by the artist or has the same style as the artist. The generative network learns to generate images to fool the discriminator into believing that the generated image is truly painted by the artist and not a fake image. However, it is well known that the traditional GAN training procedure is not particularly fit for this problem. TypicallyGANs require paired training samples to help the model learn what an output from the generator should look like. In our case, since we don’t know exactly what the output of the generator should look like since we don’t have paintings of every real life scene from every artist, one
option is to adapt to a cycle-consistent adversarial network. In particular, Cycle-GAN [1] addresses the unpaired image-to-image translation problem that we encounter in collective style transfer. The authors of Cycle-GAN simultaneously trained two pairs of generative networks and discriminative networks, one to produce imitative paintings and the other to transform the imitation back to the original photograph and pursue cycle consistency.
